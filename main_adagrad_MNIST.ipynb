{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report project: AdaGrad Optimizer Notebook\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import uself libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import useful libraries for computation\n",
    "import numpy as np\n",
    "\n",
    "# Import torch and libraries to deal with NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pyhessian import hessian # Hessian computation\n",
    "#from density_plot import get_esd_plot # ESD plot\n",
    "\n",
    "# Import usefil library to visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the LeNet5 architecture we are going to use for our study and comparisons\n",
    "from cnn_architectures import *\n",
    "\n",
    "# Importing parameters to use with different optimizers before comparing them\n",
    "import params\n",
    "\n",
    "# Importing useful functions\n",
    "from helpers import *\n",
    "\n",
    "# Ignoring warnings to make the code more readable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the parameters and additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininig neural network's parameters and seed for reproducibility purposes\n",
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = 32\n",
    "N_CLASSES = 10\n",
    "# Checking device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, reshaping and plotting  data (AdraGrad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "transforms = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "raw_mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "raw_mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
    "\n",
    "# Passing train data to the dataloader\n",
    "train_loader = DataLoader(dataset=raw_mnist_trainset, \n",
    "                          batch_size=params.ADAGRAD_BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "# Passing test data to the dataloader\n",
    "test_loader = DataLoader(dataset=raw_mnist_testset, \n",
    "                          batch_size=params.ADAGRAD_BATCH_SIZE, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping train data (from 28*28 to 32*32) for visualization purposes\n",
    "train_data, train_target = reshape_train_data(raw_mnist_trainset, DEVICE)\n",
    "# Reshaping test data (from 28*28 to 32*32) for visualization purposes\n",
    "test_data, test_target = reshape_test_data(raw_mnist_trainset, DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and Model Evaluation using AdaGrad\n",
    "\n",
    "First, we train our model using LeNet5. The model was trained using batches of size 50 and 15 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model we are going to use in our study\n",
    "model = LeNet5(num_classes=N_CLASSES)\n",
    "# Defining the criterion (loss function) to be used during the training procedure\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Defining and initializing the optimizer (AdaGrad in this notebook)\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=params.ADAGRAD_LEARNING_RATE, weight_decay=params.ADAGRAD_DECAY, initial_accumulator_value= params.ADAGRAD_INITIAL_ACCUMULATOR_VALUE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and test our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15:01:13 --- Epoch: 0\tTrain loss: 0.2773\tValid loss: 0.1552\tTrain accuracy: 95.28\tValid accuracy: 95.45\n",
      "15:01:57 --- Epoch: 1\tTrain loss: 0.1333\tValid loss: 0.1093\tTrain accuracy: 96.82\tValid accuracy: 96.78\n",
      "15:02:37 --- Epoch: 2\tTrain loss: 0.0996\tValid loss: 0.0905\tTrain accuracy: 97.50\tValid accuracy: 97.25\n",
      "15:02:57 --- Epoch: 3\tTrain loss: 0.0817\tValid loss: 0.0806\tTrain accuracy: 97.86\tValid accuracy: 97.61\n",
      "15:03:22 --- Epoch: 4\tTrain loss: 0.0697\tValid loss: 0.0689\tTrain accuracy: 98.25\tValid accuracy: 97.92\n",
      "15:03:46 --- Epoch: 5\tTrain loss: 0.0611\tValid loss: 0.0649\tTrain accuracy: 98.48\tValid accuracy: 98.10\n",
      "15:04:06 --- Epoch: 6\tTrain loss: 0.0546\tValid loss: 0.0615\tTrain accuracy: 98.64\tValid accuracy: 98.20\n",
      "15:04:27 --- Epoch: 7\tTrain loss: 0.0499\tValid loss: 0.0579\tTrain accuracy: 98.80\tValid accuracy: 98.25\n",
      "15:04:46 --- Epoch: 8\tTrain loss: 0.0457\tValid loss: 0.0536\tTrain accuracy: 98.90\tValid accuracy: 98.28\n",
      "15:05:07 --- Epoch: 9\tTrain loss: 0.0422\tValid loss: 0.0516\tTrain accuracy: 98.95\tValid accuracy: 98.35\n",
      "15:05:27 --- Epoch: 10\tTrain loss: 0.0389\tValid loss: 0.0519\tTrain accuracy: 99.02\tValid accuracy: 98.40\n",
      "15:05:49 --- Epoch: 11\tTrain loss: 0.0365\tValid loss: 0.0488\tTrain accuracy: 99.08\tValid accuracy: 98.42\n",
      "15:06:08 --- Epoch: 12\tTrain loss: 0.0340\tValid loss: 0.0480\tTrain accuracy: 99.15\tValid accuracy: 98.41\n",
      "15:06:27 --- Epoch: 13\tTrain loss: 0.0320\tValid loss: 0.0462\tTrain accuracy: 99.19\tValid accuracy: 98.41\n",
      "15:06:46 --- Epoch: 14\tTrain loss: 0.0302\tValid loss: 0.0468\tTrain accuracy: 99.32\tValid accuracy: 98.42\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, losses, grad_norms = training_loop(model, criterion, optimizer, train_loader, test_loader, params.ADAGRAD_N_EPOCHS,\n",
    "                                    DEVICE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gradient norm, which has been proven to be an important factor related to generalization properties of the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_norm(grad_norms[-30:], method = 'AdaGrad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_confusion_matrix(test_loader, model, N_CLASSES)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start looking at the eigenvalues in order to see whether we've reached a flat or sharp minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now divide the training dataset into batches to compute the hessian of the loss evaluated in the solution\n",
    "for inputs, targets in train_loader:\n",
    "    break\n",
    "\n",
    "# We move everything to the device\n",
    "inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "\n",
    "# We now compute the hessian matrix, to later retrieve the spectral norm and the eigenvalues\n",
    "device_flag = True if torch.cuda.is_available() else False\n",
    "hessian_comp = hessian(model, criterion, data=(inputs, targets), cuda=device_flag)\n",
    "\n",
    "# Now let's compute the top eigenvalue. This only takes a few seconds.\n",
    "top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues()\n",
    "print(\"The top Hessian eigenvalue of this model is %.4f\"%top_eigenvalues[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
