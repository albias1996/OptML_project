{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4596e6de",
   "metadata": {},
   "source": [
    "### Import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6619c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import useful libraries for computation\n",
    "import numpy as np\n",
    "\n",
    "# Import torch and libraries to deal with NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "# pip install torch_optimizer\n",
    "import torch_optimizer as optim\n",
    "import copy\n",
    "# Import usefil library to visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the LeNet5 architecture we are going to use for our study and comparisons\n",
    "from cnn_architectures import *\n",
    "\n",
    "# Importing parameters to use with different optimizers before comparing them\n",
    "import params\n",
    "\n",
    "# Importing useful functions\n",
    "from helpers import *\n",
    "\n",
    "# Ignoring warnings to make the code more readable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c30f6",
   "metadata": {},
   "source": [
    "### Setting the parameters and additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2abfa90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininig neural network's parameters and seed for reproducibility purposes\n",
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = 32\n",
    "N_CLASSES = 10\n",
    "# Checking device\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed83199",
   "metadata": {},
   "source": [
    "### Loading, reshaping and plotting  data (ADAHessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c8dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "transforms = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "raw_mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "raw_mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
    "\n",
    "# Passing train data to the dataloader\n",
    "train_loader = DataLoader(dataset=raw_mnist_trainset, \n",
    "                          batch_size=params.AH_BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "# Passing test data to the dataloader\n",
    "test_loader = DataLoader(dataset=raw_mnist_testset, \n",
    "                          batch_size=params.AH_BATCH_SIZE, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1405dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping train data (from 28*28 to 32*32) for visualization purposes\n",
    "train_data, train_target = reshape_train_data(raw_mnist_trainset, DEVICE)\n",
    "# Reshaping test data (from 28*28 to 32*32) for visualization purposes\n",
    "test_data, test_target = reshape_test_data(raw_mnist_trainset, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a79dda",
   "metadata": {},
   "source": [
    "## Model training and Model Evaluation using ADAHessian\n",
    "\n",
    "First, we train our model using LeNet5. The model was trained using batches of size 128 and 10 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2568f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing the model we are going to use in our study\n",
    "model = LeNet5(num_classes=N_CLASSES)\n",
    "# Defining the criterion (loss function) to be used during the training procedure\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Defining and initializing the optimizer (ADAM in this notebook)\n",
    "optimizer = optim.Adahessian(model.parameters(),\n",
    "    lr = params.AH_LEARNING_RATE,\n",
    "    betas= params.AH_BETAS,\n",
    "    eps= params.AH_EPS,\n",
    "    weight_decay= params.AH_WD,\n",
    "    hessian_power=params.AH_power)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a626a",
   "metadata": {},
   "source": [
    "Let's train and test our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f1fa14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:17:47 --- Epoch: 0\tTrain loss: 191.2571\tValid loss: 345.9440\tTrain accuracy: 11.24\tValid accuracy: 11.35\n",
      "16:18:22 --- Epoch: 1\tTrain loss: 204.9368\tValid loss: 136.9343\tTrain accuracy: 10.22\tValid accuracy: 10.10\n",
      "16:18:56 --- Epoch: 2\tTrain loss: 46.5819\tValid loss: 37.0907\tTrain accuracy: 9.75\tValid accuracy: 9.74\n",
      "16:19:31 --- Epoch: 3\tTrain loss: 25.8781\tValid loss: 18.8322\tTrain accuracy: 9.75\tValid accuracy: 9.74\n",
      "16:20:05 --- Epoch: 4\tTrain loss: 18.8863\tValid loss: 17.4562\tTrain accuracy: 9.91\tValid accuracy: 10.09\n",
      "16:20:40 --- Epoch: 5\tTrain loss: 11.6708\tValid loss: 9.8202\tTrain accuracy: 9.87\tValid accuracy: 9.80\n",
      "16:21:14 --- Epoch: 6\tTrain loss: 8.2005\tValid loss: 4.7378\tTrain accuracy: 11.24\tValid accuracy: 11.35\n",
      "16:21:49 --- Epoch: 7\tTrain loss: 2.9584\tValid loss: 2.6984\tTrain accuracy: 10.22\tValid accuracy: 10.10\n",
      "16:22:24 --- Epoch: 8\tTrain loss: 2.5337\tValid loss: 2.4626\tTrain accuracy: 9.91\tValid accuracy: 10.09\n",
      "16:22:58 --- Epoch: 9\tTrain loss: 2.5589\tValid loss: 2.5882\tTrain accuracy: 9.87\tValid accuracy: 9.80\n",
      "16:23:33 --- Epoch: 10\tTrain loss: 2.5323\tValid loss: 2.7858\tTrain accuracy: 10.44\tValid accuracy: 10.28\n",
      "16:24:07 --- Epoch: 11\tTrain loss: 2.5180\tValid loss: 2.6978\tTrain accuracy: 10.22\tValid accuracy: 10.10\n",
      "16:24:42 --- Epoch: 12\tTrain loss: 2.5317\tValid loss: 2.6370\tTrain accuracy: 9.75\tValid accuracy: 9.74\n",
      "16:25:17 --- Epoch: 13\tTrain loss: 2.4972\tValid loss: 2.5087\tTrain accuracy: 9.91\tValid accuracy: 10.09\n",
      "16:25:52 --- Epoch: 14\tTrain loss: 2.5329\tValid loss: 2.6821\tTrain accuracy: 9.75\tValid accuracy: 9.74\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, losses, grad_norms = training_loop(model, criterion, optimizer, train_loader, test_loader, params.ADAM_N_EPOCHS,\n",
    "                                    DEVICE, second_order_method = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_norm(grad_norms[-30:], method = 'ADAHessian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_confusion_matrix(test_loader, model, N_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57863931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now divide the training dataset into batches to compute the hessian of the loss evaluated in the solution\n",
    "indices = np.random.choice(len(train_data),1000)\n",
    "hessian_input, hessian_label = train_data[indices].to(DEVICE), train_target[indices].to(DEVICE)\n",
    "\n",
    "\n",
    "# We now compute the hessian matrix, to later retrieve the spectral norm and the eigenvalues\n",
    "device_flag = True if torch.cuda.is_available() else False\n",
    "model_to_plot = copy.deepcopy(model)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "hessian_comp = hessian(model_to_plot, criterion, data=(hessian_input, hessian_label), cuda=device_flag)\n",
    "\n",
    "# Now let's compute the top eigenvalue. This only takes a few seconds.\n",
    "top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(top_n=1)\n",
    "\n",
    "# Now let's compute the top 2 eigenavlues and eigenvectors of the Hessian\n",
    "print(\"The top eigenvalue of this model is: %.4f \"% (top_eigenvalues[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda is a small scalar that we use to perturb the model parameters along the eigenvectors \n",
    "lams = np.linspace(-0.5, 0.5, 21).astype(np.float32)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# At first, we initialized the perturb model to be the model obtained at the end of the training procedure\n",
    "model_perb = copy.deepcopy(model)\n",
    "\n",
    "# We now perturb the function in the direction given by the top eigenvector to visualize the quality of the minimum\n",
    "for lam in lams:\n",
    "    model_perb = get_params(model, model_perb, top_eigenvector[0], lam)\n",
    "    loss_list.append(criterion(model_perb(hessian_input), hessian_label).item())\n",
    "\n",
    "plt.plot(lams, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Perturbation')\n",
    "plt.title('Loss landscape perturbed based on top Hessian eigenvector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdf6646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhessian.utils import normalization\n",
    "\n",
    "\n",
    "# used to perturb your model \n",
    "lams = np.linspace(-0.5, 0.5, 21).astype(np.float32)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# create a copy of the model\n",
    "model_perb = copy.deepcopy(model)\n",
    "\n",
    "# generate gradient vector to do the loss plot\n",
    "loss = criterion(model_perb(hessian_input), hessian_label)\n",
    "loss.backward()\n",
    "\n",
    "v = [p.grad.data for p in model_perb.parameters()]\n",
    "v = normalization(v)\n",
    "model_perb.zero_grad()\n",
    "\n",
    "\n",
    "for lam in lams: \n",
    "    model_perb = get_params(model, model_perb, v, lam)\n",
    "    loss_list.append(criterion(model_perb(hessian_input), hessian_label).item())\n",
    "\n",
    "plt.plot(lams, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Perturbation')\n",
    "plt.title('Loss landscape perturbed based on gradient direction')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
