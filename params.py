
## Parameters for ADAM optimizer

ADAM_LEARNING_RATE = 0.001
ADAM_BATCH_SIZE = 32
ADAM_N_EPOCHS = 15


## Parameters for SGD optimizer 

SGD_LEARNING_RATE = 0.01
SGD_BATCH_SIZE = 128
SGD_N_EPOCHS = 10 

## Parameters for AdaGrad optimizer 

ADAGRAD_LEARNING_RATE = 0.01
ADAGRAD_DECAY = 0
ADAGRAD_INITIAL_ACCUMULATOR_VALUE = 0
ADAGRAD_BATCH_SIZE = 100
ADAGRAD_N_EPOCHS = 15


## Parameters for ADAHessian (AH) optimizer 

AH_LEARNING_RATE = 0.01
AH_BATCH_SIZE = 128
AH_N_EPOCHS = 10 
AH_BETAS = ...
AH_EPS = ...
AH_WD = ...
AH_POWER = ...



