{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import useful libraries\n",
    "\n",
    "This project was done in the scope of the CS-439 : Optimization for ML course. The project was done by :  \n",
    "- Brioschi Riccardo \n",
    "- Mossinelli Giacomo\n",
    "- Havolli Albias \n",
    "\n",
    "In this notebook, we focus on the analysis of the SGD optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import useful libraries for computation\n",
    "import numpy as np\n",
    "\n",
    "# Import torch and libraries to deal with NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Import pyhessian to deal with second order information\n",
    "from pyhessian import hessian # Hessian computation\n",
    "import copy\n",
    "\n",
    "# Import usefil library to visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the LeNet5 architecture we are going to use for our study and comparisons\n",
    "from cnn_architectures import *\n",
    "\n",
    "# Importing parameters to use with different optimizers before comparing them\n",
    "import params\n",
    "\n",
    "# Importing useful functions\n",
    "from helpers import *\n",
    "\n",
    "# Ignoring warnings to make the code more readable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the parameters and additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininig neural network's parameters and seed for reproducibility purposes\n",
    "RANDOM_SEED = 42\n",
    "IMG_SIZE = 32\n",
    "N_CLASSES = 10\n",
    "\n",
    "\n",
    "# Checking device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading, reshaping and plotting  data (SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "transforms = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "raw_mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "raw_mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
    "\n",
    "# Passing train data to the dataloader\n",
    "train_loader = DataLoader(dataset=raw_mnist_trainset, \n",
    "                          batch_size=params.SGD_BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "# Passing test data to the dataloader\n",
    "test_loader = DataLoader(dataset=raw_mnist_testset, \n",
    "                          batch_size=params.SGD_BATCH_SIZE, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moving data to local device in order to visualize them (and use them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping train data (from 28*28 to 32*32) for visualization purposes\n",
    "train_data, train_target = reshape_train_data(raw_mnist_trainset, DEVICE)\n",
    "# Reshaping test data (from 28*28 to 32*32) for visualization purposes\n",
    "test_data, test_target = reshape_test_data(raw_mnist_trainset, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and Model Evaluation using SGD\n",
    "\n",
    "First, we train our model using LeNet5. The model was trained using hyperparameters you can find in `helpers.py`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data to compute the stochastic approximation of the hessian and its eigenvalues\n",
    "# along the training trajectory (the choice of the batch size is fundamental, see report)\n",
    "indices = np.random.choice(len(train_data),100)\n",
    "hessian_input, hessian_label = train_data[indices].to(DEVICE), train_target[indices].to(DEVICE)\n",
    "data_for_spectral_gap = [hessian_input, hessian_label]\n",
    "\n",
    "# Initializing the model we are going to use in our study\n",
    "model = LeNet5(num_classes=N_CLASSES).to(DEVICE)\n",
    "\n",
    "# Defining the criterion (loss function) to be used during the training procedure\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining and initializing the optimizer (ADAM in this notebook)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=params.SGD_LEARNING_RATE)\n",
    "\n",
    "# Adding this variable to know the number of iterations to compute in every epoch\n",
    "iter_per_epoch = len(train_data) / params.SGD_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train and test our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, optimizer, losses, grad_norms, spectral_gap = training_loop(model, criterion, optimizer, train_loader, test_loader, params.SGD_N_EPOCHS,\n",
    "                                    DEVICE, data_for_spectral_gap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the gradient norm, which has been proven to be an important factor related to generalization properties of the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_norm(grad_norms[-30:], method = 'SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the confusion matrix in order to observe the in-class accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_confusion_matrix(test_loader, model, N_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the spectral gap to discuss the possibility of introducing second order information in the last phase of the training (see report for a better explanation and for a quick overview of the limitation of this method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectral_gap(spectral_gaps, method = 'SGD')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now start looking at the eigenvalues in order to see whether we've reached a flat or sharp minimum. The top 2 largest and smallest eigenvalue are essential in order to observe how the second order approximation behaves in the neighborhood of the point to which we have converged (we assume, as shown in the previous plot, that the first order approximation given by the gradient is approximately zero). Notice that this analysis is different from the one provided with the spectral gap plot: here we focus on the point to which we have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now divide the training dataset into batches to compute the hessian of the loss evaluated in the solution\n",
    "indices = np.random.choice(len(train_data),1000)\n",
    "hessian_input, hessian_label = train_data[indices].to(DEVICE), train_target[indices].to(DEVICE)\n",
    "\n",
    "\n",
    "# We now compute the hessian matrix, to later retrieve the spectral norm and the eigenvalues\n",
    "device_flag = True if torch.cuda.is_available() else False\n",
    "model_to_plot = copy.deepcopy(model)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "hessian_comp = hessian(model_to_plot, criterion, data=(hessian_input, hessian_label), cuda=device_flag)\n",
    "\n",
    "# Now let's compute the top eigenvalue. This only takes a few seconds.\n",
    "top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(top_n=1)\n",
    "\n",
    "# Now let's compute the top 2 eigenavlues and eigenvectors of the Hessian\n",
    "print(\"The top eigenvalue of this model is: %.4f \"% (top_eigenvalues[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the loss landscape. The plot below represents how the loss behaves in the directions given by the top eigenvector (direction of maximum perturbation and maximum stretch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lambda is a small scalar that we use to perturb the model parameters along the eigenvectors \n",
    "lams = np.linspace(-0.5, 0.5, 21).astype(np.float32)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# At first, we initialized the perturb model to be the model obtained at the end of the training procedure\n",
    "model_perb = copy.deepcopy(model)\n",
    "\n",
    "# We now perturb the function in the direction given by the top eigenvector to visualize the quality of the minimum\n",
    "for lam in lams:\n",
    "    model_perb = get_params(model, model_perb, top_eigenvector[0], lam)\n",
    "    loss_list.append(criterion(model_perb(hessian_input), hessian_label).item())\n",
    "\n",
    "plt.plot(lams, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Perturbation')\n",
    "plt.title('Loss landscape perturbed based on top Hessian eigenvector')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
