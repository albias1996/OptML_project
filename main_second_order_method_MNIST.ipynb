{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "298d169d",
   "metadata": {},
   "source": [
    "## AdaHessian Optimizer Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4596e6de",
   "metadata": {},
   "source": [
    "### Import useful libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6619c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Import useful libraries for computation\n",
    "import numpy as np\n",
    "\n",
    "# Import torch and libraries to deal with NN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# pip install torch_optimizer\n",
    "import torch_optimizer as optim\n",
    "import copy\n",
    "\n",
    "# Import pyhessian to deal with second order information\n",
    "from pyhessian import hessian # Hessian computation\n",
    "\n",
    "# Import usefil library to visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Importing the LeNet5 architecture we are going to use for our study and comparisons\n",
    "from cnn_architectures import *\n",
    "\n",
    "# Importing parameters to use with different optimizers before comparing them\n",
    "import params\n",
    "\n",
    "# Importing useful functions\n",
    "from helpers import *\n",
    "\n",
    "# Ignoring warnings to make the code more readable\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9c30f6",
   "metadata": {},
   "source": [
    "### Setting the parameters and additional variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2abfa90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defininig neural network's parameters and seed for reproducibility purposes\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "IMG_SIZE = 32\n",
    "N_CLASSES = 10\n",
    "\n",
    "# Checking device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed83199",
   "metadata": {},
   "source": [
    "### Loading, reshaping and plotting  data (ADAHessian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19c8dd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "transforms = transforms.Compose([transforms.Resize(IMG_SIZE),\n",
    "                                 transforms.ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "raw_mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms)\n",
    "raw_mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms)\n",
    "\n",
    "# Passing train data to the dataloader\n",
    "train_loader = DataLoader(dataset=raw_mnist_trainset, \n",
    "                          batch_size=params.AH_BATCH_SIZE, \n",
    "                          shuffle=True)\n",
    "\n",
    "# Passing test data to the dataloader\n",
    "test_loader = DataLoader(dataset=raw_mnist_testset, \n",
    "                          batch_size=params.AH_BATCH_SIZE, \n",
    "                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5daa47",
   "metadata": {},
   "source": [
    "### Moving data to local device in order to visualize them (and use them later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c1405dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping train data (from 28*28 to 32*32) for visualization purposes\n",
    "train_data, train_target = reshape_train_data(raw_mnist_trainset, DEVICE)\n",
    "# Reshaping test data (from 28*28 to 32*32) for visualization purposes\n",
    "test_data, test_target = reshape_test_data(raw_mnist_trainset, DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ec3f81",
   "metadata": {},
   "source": [
    "We now visualize the data, to let the reader become familiar with the data and the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109f0ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting some training examples with ground truth \n",
    "figure = plt.figure(figsize=(10, 8))\n",
    "cols, rows = 5, 5\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_loader), size=(1,)).item()\n",
    "    img = train_data[sample_idx]\n",
    "    label = train_target[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(\"Ground Truth: {}\".format(train_target[sample_idx]))\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap=\"gray\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a79dda",
   "metadata": {},
   "source": [
    "## Model training and Model Evaluation using AdaHessian\n",
    "\n",
    "First, we train our model using LeNet5. The model was trained using hyperparameters you can find in `params.py`. Despite the low number of training episodes and iterations, the model seems to perform well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2568f77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining data to compute the stochastic approximation of the hessian and its eigenvalues\n",
    "# along the training trajectory (the choice of the batch size is fundamental, see report)\n",
    "indices = np.random.choice(len(train_data),100)\n",
    "hessian_input, hessian_label = train_data[indices].to(DEVICE), train_target[indices].to(DEVICE)\n",
    "data_for_spectral_gap = [hessian_input, hessian_label]\n",
    "\n",
    "# Initializing the model we are going to use in our study\n",
    "model = LeNet5(num_classes=N_CLASSES)\n",
    "\n",
    "# Defining the criterion (loss function) to be used during the training procedure\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Defining and initializing the optimizer (ADAM in this notebook)\n",
    "optimizer = optim.Adahessian(model.parameters(),\n",
    "    lr = params.AH_LEARNING_RATE,\n",
    "    betas= params.AH_BETAS,\n",
    "    eps= params.AH_EPS,\n",
    "    weight_decay= params.AH_WD,\n",
    "    hessian_power=params.AH_power)\n",
    "\n",
    "\n",
    "# define nb. per epoch using bathc size \n",
    "iter_per_epoch = len(train_data) / params.AH_BATCH_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035a626a",
   "metadata": {},
   "source": [
    "Let's train and test our first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35f1fa14",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:17:47 --- Epoch: 0\tTrain loss: 191.2571\tValid loss: 345.9440\tTrain accuracy: 11.24\tValid accuracy: 11.35\n",
      "16:18:22 --- Epoch: 1\tTrain loss: 204.9368\tValid loss: 136.9343\tTrain accuracy: 10.22\tValid accuracy: 10.10\n",
      "16:18:56 --- Epoch: 2\tTrain loss: 46.5819\tValid loss: 37.0907\tTrain accuracy: 9.75\tValid accuracy: 9.74\n",
      "16:19:31 --- Epoch: 3\tTrain loss: 25.8781\tValid loss: 18.8322\tTrain accuracy: 9.75\tValid accuracy: 9.74\n",
      "16:20:05 --- Epoch: 4\tTrain loss: 18.8863\tValid loss: 17.4562\tTrain accuracy: 9.91\tValid accuracy: 10.09\n",
      "16:20:40 --- Epoch: 5\tTrain loss: 11.6708\tValid loss: 9.8202\tTrain accuracy: 9.87\tValid accuracy: 9.80\n",
      "16:21:14 --- Epoch: 6\tTrain loss: 8.2005\tValid loss: 4.7378\tTrain accuracy: 11.24\tValid accuracy: 11.35\n",
      "16:21:49 --- Epoch: 7\tTrain loss: 2.9584\tValid loss: 2.6984\tTrain accuracy: 10.22\tValid accuracy: 10.10\n",
      "16:22:24 --- Epoch: 8\tTrain loss: 2.5337\tValid loss: 2.4626\tTrain accuracy: 9.91\tValid accuracy: 10.09\n",
      "16:22:58 --- Epoch: 9\tTrain loss: 2.5589\tValid loss: 2.5882\tTrain accuracy: 9.87\tValid accuracy: 9.80\n",
      "16:23:33 --- Epoch: 10\tTrain loss: 2.5323\tValid loss: 2.7858\tTrain accuracy: 10.44\tValid accuracy: 10.28\n",
      "16:24:07 --- Epoch: 11\tTrain loss: 2.5180\tValid loss: 2.6978\tTrain accuracy: 10.22\tValid accuracy: 10.10\n",
      "16:24:42 --- Epoch: 12\tTrain loss: 2.5317\tValid loss: 2.6370\tTrain accuracy: 9.75\tValid accuracy: 9.74\n",
      "16:25:17 --- Epoch: 13\tTrain loss: 2.4972\tValid loss: 2.5087\tTrain accuracy: 9.91\tValid accuracy: 10.09\n",
      "16:25:52 --- Epoch: 14\tTrain loss: 2.5329\tValid loss: 2.6821\tTrain accuracy: 9.75\tValid accuracy: 9.74\n"
     ]
    }
   ],
   "source": [
    "model, optimizer, losses, grad_norms, spectral_gaps = training_loop(model, criterion, optimizer, train_loader, test_loader, params.AH_N_EPOCHS,\n",
    "                                    DEVICE,data_for_spectral_gap,iter_per_epoch,second_order_method = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae5c39e",
   "metadata": {},
   "source": [
    "Plot the gradient norm, which has been proven to be an important factor related to generalization properties of the architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gradient_norm(grad_norms[-30:], method = 'ADAHessian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f3fd6",
   "metadata": {},
   "source": [
    "Plot the confusion matrix in order to observe the in-class accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8698eff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_confusion_matrix(test_loader, model, N_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df74406b",
   "metadata": {},
   "source": [
    "Plot the spectral gap to discuss the possibility of introducing second order information in the last phase of the training (see report for a better explanation and for a quick overview of the limitation of this method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101652a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_spectral_gap(spectral_gaps, method = 'AdaHessian')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8693e5",
   "metadata": {},
   "source": [
    "We now start looking at the eigenvalues in order to see whether we've reached a flat or sharp minimum. The top 2 largest and smallest eigenvalue are essential in order to observe how the second order approximation behaves in the neighborhood of the point to which we have converged (we assume, as shown in the previous plot, that the first order approximation given by the gradient is approximately zero). Notice that this analysis is different from the one provided with the spectral gap plot: here we focus on the point to which we have converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57863931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the training dataset into batches to compute the hessian of the loss evaluated in the solution\n",
    "indices = np.random.choice(len(train_data),1000)\n",
    "hessian_input, hessian_label = train_data[indices].to(DEVICE), train_target[indices].to(DEVICE)\n",
    "\n",
    "\n",
    "# Computing the hessian matrix, to later retrieve the spectral norm and the eigenvalues\n",
    "device_flag = True if torch.cuda.is_available() else False\n",
    "model_to_plot = copy.deepcopy(model)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "hessian_comp = hessian(model_to_plot, criterion, data=(hessian_input, hessian_label), cuda=device_flag)\n",
    "\n",
    "# Computing the top eigenvalue. This only takes a few seconds.\n",
    "top_eigenvalues, top_eigenvector = hessian_comp.eigenvalues(top_n=1)\n",
    "\n",
    "# Computing the top 2 eigenavlues and eigenvectors of the Hessian\n",
    "print(\"The top eigenvalue of this model is: %.4f \"% (top_eigenvalues[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e0216",
   "metadata": {},
   "source": [
    "We now plot the loss landscape. The plot below represents how the loss behaves in the directions given by the top eigenvector (direction of maximum perturbation and maximum stretch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0662abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lambda is a small scalar that we use to perturb the model parameters along the eigenvectors \n",
    "lams = np.linspace(-0.5, 0.5, 21).astype(np.float32)\n",
    "\n",
    "loss_list = []\n",
    "\n",
    "# At first, we initialized the perturb model to be the model obtained at the end of the training procedure\n",
    "model_perb = copy.deepcopy(model)\n",
    "\n",
    "# We now perturb the function in the direction given by the top eigenvector to visualize the quality of the minimum\n",
    "for lam in lams:\n",
    "    model_perb = get_params(model, model_perb, top_eigenvector[0], lam)\n",
    "    loss_list.append(criterion(model_perb(hessian_input), hessian_label).item())\n",
    "\n",
    "plt.plot(lams, loss_list)\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Perturbation')\n",
    "plt.title('Loss landscape perturbed based on top Hessian eigenvector')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
